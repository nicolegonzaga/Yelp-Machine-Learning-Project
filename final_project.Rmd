---
title: "Yelp Machine Learning Project"
author: "Nicole Gonzaga"
output:
  html_document:
      toc: true
      toc_float: true
      code_folding: show
  pdf_document: default
date: "2022-12-08"
---

# Introduction

Whether people are looking for the best sushi restaurant in town, the best dentist, or hair stylist, Yelp can be a great tool for people to find professionals and businesses close by. Yelp is an American company that developed the ***Yelp***.com website, which publishes user reviews about different categories of businesses and restaurants.

Yelp users are also able to read and react to reviews that were left by customers, can submit reviews on businesses' products or services, and rate the business on a scale of 1 to 5 stars. Businesses can also benefit from yelp reviews. Users who leave reviews establish credibility for the business. The data set can be found here: <https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset>

## ![](yelp-logo-3.png)Purpose

I chose to do my machine learning project on this Yelp data set because I'm a very active Yelp user, and always look for new restaurants to try through Yelp. The goal of this project is to build a statistical learning model that can predict the star ratings of a review based on review text and the cool, useful, and funny ratings. Another goal of this project is to understand and apply the use of Text Mining. I researched the process of Text Mining, and applied it in this project so that the models can use the review text as a predictor variable.

# Exploratory Data Analysis

```{r, message = FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(tidytext)
library(ggthemes)
library(glue)
library(stringr)
library(ggfortify)
library(corrplot)
```

```{r}
# Loading the yelp data set
business <- read.csv(file = "yelp.csv")
```

To start the EDA process, I extracted the length of each review in the 'text' column and created a new column called 'review_length'.

```{r, message = FALSE}
library(stringr)
business1 <- read.csv(file = "yelp.csv")
business1$review_length = str_length(business1$text)
```

#### Stars and count

```{r}
par(mfrow=c(2,2))
#stars vs count
ggplot(business1, aes(x = factor(stars), fill= factor(stars))) +                           geom_bar() + 
                      theme_minimal() + 
                      ggtitle("Plot of Stars vs Count")
```

### Review length vs count

```{r, warning= FALSE}
ggplot(business1, aes(x=review_length, fill= factor(stars))) + geom_histogram() + facet_wrap(~stars) + ggtitle("Review Length vs. Count")
```

There seems to be a higher amount of 4 and 5 star reviews than 3, 2 or 1 stars. Longer review lengths also are associated with higher stars. This could mean that yelp users mostly write yelp reviews when they had a positive experience at the business.

### Cool, Useful, Funny Ratings on Reviews

```{r}
par(mfrow=c(1,3))
#cool vs count
cool <- ggplot(business1, aes(x = cool)) + geom_bar(color="dark green")

#useful vs count 
useful <- ggplot(business1, aes(x = useful)) + geom_bar(color="red")

#funny vs count
funny <- ggplot(business1, aes(x = funny)) + geom_bar(color="blue")

library(gridExtra)
grid.arrange(cool, useful, funny, ncol=3, nrow =1)
```

These next graphs are for each of the review ratings: cool, useful, and funny (CUF for short). CUF is Yelp's alternative to the traditional "like" or "thumbs up" button that are seen on other platforms such as YouTube or Facebook. Based on these graphs, it seems like not many users use the review ratings as the highest count is at 0 user ratings.

### Text Mining/Sentiment Analysis

Next, I used a text mining process to extract the sentiment words in the review text. The first step is to tokenize the text. Tokenization is essentially the process of taking a string (in our case, it's the 'text' variable) and splitting the text into separate pieces or "tokens".

After tokenizing, we would need to remove any stop words in order to get the sentiment words, which are commonly used words that are found in sentences, for example, "a", "the", "are", etc.

```{r}
#Tokenizing the text and assigning each token to the word column
set.seed(426)
business_tokens <- business1 %>%
  unnest_tokens(word, text)

library(stopwords)
business_tokens <- business_tokens %>%
  filter(!(word %in% stopwords(source = "snowball")))
```

```{r}
# The first review in the data set after tokenizing and removing stop words
head(business_tokens$word, 10)
```

Here I used a premade sentiment dictionary called "bings" to categorize the words into positive or negative words.

```{r, message = FALSE}
# get the sentiment
business2 <- business_tokens %>%
  inner_join(get_sentiments("bing")) %>%
  ungroup()
```

### Comparing positive and negative words

```{r, message = FALSE}
# get the sentiment
business_sentiment1 <- business2 %>%
  inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
  count(word, sentiment, sort= TRUE) %>% # count the # of positive & negative words
  ungroup() 

business_sentiment1 %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) + 
  geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = "free_y") + coord_flip()
```

The reviews have more positive words than negative, which makes sense since we showed that higher stars are associated with more reviews, and the review lengths are also longer for highly reviewed ratings.This could also mean that most yelp users write reviews after having a positive experience at the business.

\newpage

# Data Splitting

I split the data into training and testing sets with a proportion of 0.8 and strata set to type1. I created a recipe, which will be used for model fitting in the next section. I also tokenized the recipe in order to use it for model fitting. Stratified sampling is used to avoid sampling bias.

```{r}
#Making the response variable a factor variable with 5 levels. 1-5 stars
business_class <- business %>% 
mutate(type1 = factor(stars)) 
```

```{r}
#Splitting the data
business_split <- initial_split(business_class, strata = type1, prop = 0.8)
business_train <- training(business_split)
business_test <- testing(business_split)

#Creating recipe
business_rec <-
  recipe(type1 ~ text + cool + useful + funny, data = business_train)

#Creating k-fold variable
set.seed(234)
business_folds <- vfold_cv(business_train, v=5, strata = type1)
```

```{r}
library(textrecipes)
business_rec <- business_rec %>%
  step_tokenize(text) %>%
  step_tokenfilter(text) %>%
  step_tfidf(text)

#Setting up workflow
business_wf <- workflow() %>%
  add_recipe(business_rec)
```

# Model Fitting

### Model 1: Naive Bayes model

First, I fit a Naive Bayes Model to the training data. I specified the engine to be a Naive Bayes model, created a workflow, and fit the model to the training data.

```{r, message = FALSE}
library(klaR)
library(MASS)
library(discrim)
library(naivebayes)
```

```{r, message = FALSE}
#Engine specification
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

#Creating a workflow 
nb_fit <- business_wf %>%
  add_model(nb_spec) %>%
  fit(data = business_train)

nb_wf <- workflow() %>%
  add_recipe(business_rec) %>%
  add_model(nb_spec)
```

```{r, eval=FALSE}
nb_rs <- fit_resamples(
  nb_wf,
  business_folds,
  control = control_resamples(save_pred = TRUE)
)

#Saved the results so that I would not have to run it again
save(nb_rs, file = "nb_rs_results.rda")
```

```{r}
load(file = "nb_rs_results.rda")

#ROC_AUC and Accuracy of  the model
nb_rs_metric <- collect_metrics(nb_rs)%>% 
  arrange(desc(mean)) %>% 
  slice(1)
nb_rs_metric


```

The ROC AUC of the Naive Bayes model has a mean of 0.5462. This means that the model is only able to accurately classify the data about 55% of the time. The Naive Bayes model's predictive ability is only as good as a random guess or a coin flip.

### Model 2: Multinomial Logistic regression

For this Multinomial Logistic Regression model, I used an elastic net tuning method and tuned penalty and mixture with 10 levels.

```{r}
#Creating spec
multinom_spec <- multinom_reg(mixture = tune(), penalty = tune()) %>%
  set_engine("glmnet")

#Creating workflow
multinom_wf <- workflow() %>%
  add_recipe(business_rec) %>%
  add_model(multinom_spec)

# Creating a regular grid with 10 levels
penalty_grid <- grid_regular(penalty(range = c(-5,5)), mixture(range=c(0,1)), levels = 10)
```

```{r, eval= FALSE}
#Fit models to your folded data using tune_grid()
tune_res <- tune_grid(
  multinom_wf,
  resamples = business_folds,
  grid= penalty_grid
)
```

```{r, eval=FALSE}
#save(tune_res, file = "business_tune_res")
```

```{r}
load(file="business_tune_res")

autoplot(tune_res, metric = "roc_auc")

tune_best <- tune_res %>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
tune_best
```

The best performing is a model with penalty = 0.00012155, mixture = 1.0, and mean = 0.7313.

### Model 3: Random forest

The next model is a Random Forest. I also followed the same process as the previous models and created a spec, workflow and penalty grid. For the parameters, I chose mtry = 1 to 5, min_n = 5 to 20 and trees = 100 to 300

```{r}
#Creating spec anf setting engine to ranger
rf_mod <- 
  rand_forest(
              min_n = tune(),
              mtry = tune(),
              trees = tune(),
              mode = "classification") %>% 
  set_engine("ranger", importance = "impurity")

#Creating workflow
rf_workflow <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(business_rec)

#Creating grid parameter
param_grid <- grid_regular(mtry(range = c(1,5)), min_n(range = c(5,20)),
                           trees(range = c(100, 300)),
                           levels = 5)
```

```{r, eval = FALSE}
library(ranger)
rf_tune <- tune_grid(
  rf_workflow, 
  resamples = business_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)
```

```{r, eval=FALSE}
#Saving rf_tune so that I dont have to run it again
save(rf_tune, file = "rf_tune.rda") #Loading rf_tune
```

```{r}
#Loading rf_tune
load(file = "rf_tune.rda")

autoplot(rf_tune, metric = "roc_auc")

rf_best <- rf_tune %>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
rf_best
```

The random forest model took the longest to run out of all 4. The top model seems to be the model with mtry = 4, trees=300, and mean = 0.7100289. This means that the Random forest model is fairly good at predicting star ratings. Also, random forest model is much better at predicting than the Naive Bayes Model.

### Model 4: Boosted Tree

Lastly, I fit a Boosted tree following similar steps to the previous models. For mtry, I used a range of 1 to 5, and the a range of -5 to 0.2 for the learn rate.

```{r}
bt_model <- boost_tree(mode = "classification",
                       min_n = tune(),
                       mtry = tune(),
                       learn_rate = tune()) %>% 
  set_engine("xgboost")

bt_workflow <- workflow() %>% 
  add_model(bt_model) %>% 
  add_recipe(business_rec)
```

```{r}
bt_params <- parameters(bt_model) %>% 
  update(mtry = mtry(range= c(1, 5)),
         learn_rate = learn_rate(range = c(-5, 0.2))
  )

# define grid
bt_grid <- grid_regular(bt_params, levels = 5)
```

```{r, eval=FALSE}
#Tuning grid
bt_tune <- bt_workflow %>% 
  tune_grid(
    resamples = business_folds, 
    grid = bt_grid
    )
```

```{r, eval=FALSE}
#Saving bt_tune
save(bt_tune, file = "bt_tune.rda")
```

```{r}
#Loading bt_tune
load(file="bt_tune.rda")

#Learning rate autoplot of bt_tune
autoplot(bt_tune, metric = "roc_auc")

#Selecting the best boosted tree model
bt_best <- bt_tune %>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(1)
bt_best
```

The best performing Boosted Tree model is with parameters mtry=5, min_n=2 and mean=0.69331

# Model performance

The Naive Bayes model performed the worst, which is not surprising as this model is known to be "bad" estimator. Our best performing model is the Multinomial Logistic Regression model. So we can use this model to fit to our testing set.

```{r}
model_rocauc <- c(nb_rs_metric$mean, tune_best$mean, bt_best$mean, rf_best$mean)

models <- c("Naive Bayes", "Multinomial Logistic Regression", "Boosted Tree", "Random Forest")

results <- tibble(roc_auc = model_rocauc, Models = models)
results %>% 
  arrange(-roc_auc)
```

# Analysis of the Test Set

```{r}
#Selecting the best multinom model
best_penalty <- select_best(tune_res, metric="roc_auc")
business_final <- finalize_workflow(multinom_wf, best_penalty)
business_final_fit <- fit(business_final, data = business_train)

set.seed(123)
final_results <- augment(business_final_fit, new_data = business_test) %>% 
  dplyr::select(type1, starts_with(".pred")) %>% 
  roc_auc(truth= type1, estimate = .pred_1:.pred_5)
final_results
```

Our final ROC AUC value on the testing set is 0.7299304. Which is slightly greater than the ROC AUC value on the training set.

### ROC AUC plots

Next, we can visualize the ROC AUC value by creating an ROC AUC plot and Confusion matrix of the testing set.

```{r}
augment(business_final_fit, new_data = business_test) %>% 
  dplyr::select(type1, starts_with(".pred")) %>% roc_curve(type1, .pred_1:.pred_5) %>% 
  autoplot()
```

### Confusion Matrix

```{r}
augment(business_final_fit, new_data = business_test) %>% conf_mat(type1, .pred_class) %>% 
  autoplot(type="heatmap")
```

# Conclusion

Based on the ROC AUC graphs, the final model is able to predict 1 star reviews the best, and 4 star reviews the worst. For predicting 2,3, and 5 star ratings, the graphs seem very similar. I think that the model can predict 1 star ratings certainly well because the sentiment words for 1 star ratings are the easiest to differentiate based on the most negative sentiment words. In addition, the model is not able to predict 4 star reviews very well because some sentiment words could be very similar to words that are considered 3 or 5 stars.

We can further prove this point by looking at the Confusion Matrix. It's clear that the final model is able to predict a lot of 4s and 5s. However, it's also mistaking 4's as 5 star ratings, and confusing 5's as 4 star ratings. The model seems to also be mistaking 2's and 3's as 4 star ratings, and so on.

The best performing model was the Multinomial Logistic Regression model. When fitting the model to the testing set, the ROC AUC value for the testing set was higher than the value for the training set. A possible reason for this could be that the data is imbalanced classification data. In the near future, I would want to resolve this problem by trying a different resampling method. While stratified sampling was used just for this project, I think that using a different sampling technique, such as oversampling or undersampling could potentially solve the imbalanced aspect.
